# Machine Learning

- Goal of Machine Learning
- Linear Classification
- Solution

## Goal of Machine Learning 机器学习的目标

假设现在有一组数据${x_i,y_i}$，其中$x_c \in \R^d$，d指的是特征数，而$y_c \in \R$是**标签值(label)**。

上述数据被称为**训练集(training data set)**。而机器学习的目的就是训练一个**模型(model)**（或者假说）$h$在某种条件下最贴近该训练集数据。

现在假设出现了一个新的点$x* \in \R^d$，我们需要用我们的模型去预测其标签值$y*$，这个值$x*$就被称作**检验数据(test data)**，模型检测标签值的准确程度被叫做**泛化误差(generalization error)**。

## Linear Classification 线性分类

上述情景的一个经典例子是线性分类。

条件：在平面上有一堆红色的点和黑色的点。

目标：找到一条直线，使得所有红色的点都在直线一侧，而黑色点都在直线另一侧。

我们保证这个直线是存在的，如何找到满足条件的直线呢？

我们将点到直线的垂直距离记为模型的标签值，并且希望所有红色点的垂直距离为正，而黑色点的垂直距离为负，这样他们就一定分布在直线的异侧。

因此我们得到训练集：
$$
    (x_1,0),(x2,1),\dotsb
$$
其中标签值为0表示红色点，为1表示黑色点。

目标：我们将所有的$x_i$丢到模型里面，模型给出的标签值可以和训练集的标签值尽量一致。

那么我们如何找到这个模型$h$呢?

## Solution 解决办法

平面，直线，你想到了我们之前学过的什么东西？没错，线性规划。

所有的红色点和黑色点都对应一个约束条件，而我们的目标是寻找可行域。

实际上我们会有无数条直线满足上面的约束条件，我们如何定义其中最好的一条决定了我们如何训练模型。我们给出的答案是，有**最大边界(maximum margin)**的一条直线。也就是说，所有的点到直线的距离都大于一个常数$\sigma$，这个$\sigma$就是边界。

上面的最优化模型也有一个名称：**支持向量机(support vector machine,SVM)**
我们使用二分搜索来确定$\sigma$，而对于每一个$\sigma$我们解一个线性规划即可。

## Numerical output example: linear regression 数值输出：线性回归

在更多的情况下，我们需要返回一个预测值，一个常见的例子就是线性回归。
我们定义了一系列训练集$(x_i,y_i)$和损失函数$L(h) = \frac{1}{n}\Sigma(<x_i,h> - y_i)^2$，
模型生成之后我们给出测试集$x*$，模型给出预测值$y*$。损失函数计算预测值和实际值的垂直距离，使得模型可以持续优化。

如何找到线性回归的模型呢？前面我们提到的梯度下降是一个好方法。

我们回忆一下梯度下降的方法：

1. 选择初始点$h_0$，步数$T$和学习率$\eta$。
2. 在每步迭代中，计算当前点的梯度，并且迭代点$h_{i+1} = h_i -\eta \nabla L(h)$
3. 最后输出$\frac{1}{T}\Sigma h_i$
(或者直接输出$h_T$)

我们发现$L(h)$具有一个很好的性质：由于$x^2$是凸函数，因此其线性组合也是凸的。所以我们可以在这个问题中使用梯度下降法。

另外一个问题是：$h_i$的梯度是什么?

要解决这个问题，我们需要关注损失函数$L(h)$的梯度